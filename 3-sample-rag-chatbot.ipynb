{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89320d6d-ef68-4fbb-80a5-8a0482d40b77",
   "metadata": {},
   "source": [
    "# Questions and answers RAG Chatbot\n",
    "\n",
    "We can use the power of LLM to get answers from our own dataset. This is called retrieval augmented generation (RAG), as you would retrieve the relevant data and use it as augmented context for the LLM. Instead of relying solely on knowledge derived from the training data, a RAG workflow pulls relevant information and connects static LLMs with data retrieval sources (VectorDBs).\n",
    "\n",
    "In the following example we will create a chatbot with Gradio. Use self hosted Qdrant Vector DB to store document embeddings that will be used in the RAG pipeline.\n",
    "\n",
    "**Note:** For embeddings it's strongly recommended to create an account on Cohere's website: https://dashboard.cohere.com/welcome/login?redirect_uri=%2Fapi-keys and generate a `Trial key`.\n",
    "\n",
    "We will start by installing the preprequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e51e9-655e-4cd7-acdb-51665923e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install openai gradio langchain_qdrant langchain-openai pypdf langchain_cohere==0.1.9 langchain-huggingface langchain_experimental langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdf1ea-d561-4bdb-ae21-89f6fe0e6725",
   "metadata": {},
   "source": [
    "Start the application and click on the public URL to open the application in a new tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd6b0c2f-cbd2-4800-b3e0-98de6822cd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered model is: meta/llama3-8b-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/gradio/layouts/column.py:55: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7872\n",
      "Running on public URL: https://c50696e01b772eab3a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c50696e01b772eab3a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: /home/jovyan/examples/data/Node Name Research - Kubernetes Engine - OCI Confluence.pdf\n",
      "File saved to: /home/jovyan/examples/data/Jabra Elite 3 Active User Manual_EN_English_RevB.pdf\n"
     ]
    }
   ],
   "source": [
    "from gradio_chatbot import setup_chatbot, create_empty_collections, clean_up_vector_db\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from openai import OpenAI\n",
    "\n",
    "QDRANT_URL = \"http://qdrant:6333\"\n",
    "\n",
    "embedding_models = {\n",
    "    \"1-nv-embedqa-e5-v5\": {\n",
    "        \"model\": \"nvidia/nv-embedqa-e5-v5\",\n",
    "        \"embedding_function\": lambda api_key: NVIDIAEmbeddings(\n",
    "            base_url = \"http://nemo-embedding-ms:8080/v1\",\n",
    "            model=\"nvidia/nv-embedqa-e5-v5\"\n",
    "        ),\n",
    "        \"size\": 1024\n",
    "    },\n",
    "    \"4-all-roberta-large-v1_1024d\": {\n",
    "        \"model\": \"sentence-transformers/all-roberta-large-v1\",\n",
    "        \"embedding_function\": lambda api_key: HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-roberta-large-v1\"\n",
    "        ),\n",
    "        \"size\": 1024\n",
    "    },\n",
    "    \"3-all-mpnet-base-v2_768d\": {\n",
    "        \"model\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"embedding_function\": lambda api_key: HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "        ),\n",
    "        \"size\": 768\n",
    "    },\n",
    "    \"2-cohere\": {\n",
    "        \"model\": \"embed-english-light-v3.0\",\n",
    "        \"embedding_size\": 384,\n",
    "        \"embedding_function\": lambda api_key: CohereEmbeddings(\n",
    "            cohere_api_key=api_key,\n",
    "            model=\"embed-english-light-v3.0\"\n",
    "        ),\n",
    "        \"size\": 384\n",
    "    }\n",
    "}\n",
    "\n",
    "llm_models = {\n",
    "    \"nVidia NIM llama3-70b-instruct\": {\n",
    "        \"model\": \"meta/llama3-70b-instruct\",\n",
    "        \"llm_function\": lambda api_key, kwargs={}: ChatOpenAI(\n",
    "            base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "            model=\"meta/llama3-70b-instruct\",\n",
    "            api_key=api_key,\n",
    "            **kwargs\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "# Attempt to fetch available models (for NIM deployments)\n",
    "try:\n",
    "    model = \"\"\n",
    "    client = OpenAI(\n",
    "      base_url = \"http://llm/v1\",\n",
    "      api_key = \"dummy\"\n",
    "    )\n",
    "    available_models = client.models.list()\n",
    "    if len(available_models.data):\n",
    "        model = available_models.data[0].id\n",
    "        print(f\"Discovered model is: {model}\")\n",
    "    else:\n",
    "        print(\"No model discovered\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "if model:\n",
    "    llm_models[f'1-{model}']= {\n",
    "        \"model\": model,\n",
    "        \"llm_function\": lambda api_key, kwargs={}: ChatOpenAI(\n",
    "            base_url=\"http://llm/v1\",\n",
    "            model=model,\n",
    "            api_key=\"dummy\",\n",
    "            **kwargs\n",
    "        )\n",
    "    }\n",
    "else:\n",
    "    llm_models[\"1-Meta-Llama-3-8B-Instruct\"]= {\n",
    "        \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        \"llm_function\": lambda api_key, kwargs={}: ChatOpenAI(\n",
    "            base_url=\"http://llm/v1\",\n",
    "            model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            api_key=api_key,\n",
    "            **kwargs\n",
    "        )\n",
    "    }\n",
    "\n",
    "clean_up_vector_db(QDRANT_URL)\n",
    "create_empty_collections(QDRANT_URL, embedding_models)\n",
    "demo = setup_chatbot(llm_models, embedding_models, QDRANT_URL)\n",
    "demo.queue().launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073752e-6b7a-4de0-911d-58876ef864a3",
   "metadata": {},
   "source": [
    "How the application works:\n",
    "\n",
    "1. Select the LLM and fill-in the LLM API Key\n",
    "2. Select the embedding model and fill-in the embedding API Key (if required)\n",
    "3. Click Load Model\n",
    "\n",
    "If the model is loaded successfuly, you should see: '<selected llm model>' and '<selected embeddings model>' models loaded\n",
    "\n",
    "4. Upload a document that will be used for RAG (txt, pdf are supported)\n",
    "5. Click Create Vector Store.\n",
    "6. Use the chatbot interface to interact with the LLM.\n",
    "7. If you change any Text Generation parameter, you have to Click \"Load Model\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
